# MultiModal Machine Learning
Track the trend of Representation learning of MultiModal Machine Learning(MMML).


## 1. Paper

### 2021
1. [CVPR oral] Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning [paper](https://arxiv.org/abs/2104.03135) [code](https://github.com/researchmm/soho)

2. [ICML] ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision [paper](https://arxiv.org/abs/2102.03334) [code](https://github.com/dandelin/ViLT)


### 2020
1. [NeurIPS] Large-Scale Adversarial Training for Vision-and-Language Representation Learning [paper](https://arxiv.org/abs/2006.06195) [code](https://github.com/zhegan27/VILLA)


### 2019
1. [NeurIPS] ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks [paper](https://arxiv.org/abs/1908.02265) [code](https://github.com/jiasenlu/vilbert_beta)

2. [EMNLP] LXMERT: Learning Cross-Modality Encoder Representations from Transformers [paper](https://arxiv.org/abs/1908.07490) [code](https://github.com/airsplay/lxmert)

3. [arXiv] VisualBERT: A Simple and Performant Baseline for Vision and Language [paper](https://arxiv.org/abs/1908.03557) [code](https://github.com/uclanlp/visualbert)


### 2018
1. [TPAMI] Multimodal machine learning: A survey and taxonomy [paper](https://arxiv.org/abs/1705.09406)


## 2. Dataset
#### 1.



## 3. Others
#### 1. few-shot-object-detection
* [website](https://github.com/pliang279/awesome-multimodal-ml)
* Reading list for research topics in multimodal machine learning
